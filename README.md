# ğŸ“Š Cloud Computing Group Assignment_1

## ğŸ‘¨â€ğŸ’» Team Members
- **EG/2020/4248**
- **EG/2020/3893**
- **EG/2020/3851**

---

## ğŸ“ Project Overview

This project implements the **WordCount MapReduce** job using **Hadoop**, applied to the **Enron Email Dataset**. The WordCount job is a foundational example in big data processing â€” showcasing how large volumes of data can be processed **in parallel** across a cluster using the **MapReduce** paradigm.

---

## ğŸ“‚ Dataset: Enron Email Dataset

- Contains approximately **500,000 emails**.
- Emails include **sender**, **receiver**, **subject**, and **body** fields.
- Originally released by the **Federal Energy Regulatory Commission (FERC)** during the Enron Corporation investigation.
- link of the data set
  https://www.kaggle.com/datasets/wcukierski/enron-email-dataset
  
---

## âš™ï¸ MapReduce Workflow

### 1ï¸âƒ£ Input
- **Source:** Raw textual email files.
- **Storage:** Hadoop Distributed File System (HDFS).

### 2ï¸âƒ£ Splitting
- Hadoop automatically splits the dataset into smaller chunks for distributed processing.

### 3ï¸âƒ£ Mapper Phase
Each Mapper:
- Reads email text.
- Extracts text from **subject** and **body** fields.
- Tokenizes into words.
- Emits `(word, 1)` key-value pairs.

### 4ï¸âƒ£ Shuffle & Sort
- Intermediate results are grouped by key (word).
- All occurrences of a word are collected and forwarded to the same Reducer.

### 5ï¸âƒ£ Reducer Phase
Each Reducer:
- Receives grouped `(word, 1)` pairs.
- Aggregates word counts using summation.
- Emits final output: `(word, total_count)`.

### 6ï¸âƒ£ Output
- The final word counts are written back to HDFS for further analysis or usage.
- Here is the structure of the final output
  
![Screenshot 2025-06-05 162955](https://github.com/user-attachments/assets/d6fc5c92-ec1f-481a-80c3-b734becf0425)

---

## ğŸš€ Technologies Used
- **Apache Hadoop**
- **MapReduce Programming Model**
- **HDFS (Hadoop Distributed File System)**
- **Java (depending on your implementation)**

---

## ğŸ“Œ Objective
The primary objective of this MapReduce process is to **efficiently analyze the Enron email dataset (~500,000 emails)** using **Hadoop's distributed computing capabilities**. Specifically, the goal is to **identify and count the frequency of each unique word** across the email dataset, demonstrating Hadoopâ€™s effectiveness in handling **large-scale text processing** tasks.

This project aims to showcase the power of **distributed computing** and **parallel processing** by applying the **MapReduce model** to analyze a real-world dataset at scale.

---


